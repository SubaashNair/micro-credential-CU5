{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Getting \"Ready\" - Data Cleaning & Preparation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook demonstrates **Module 1-2: Tame the Mess**\n",
    "\n",
    "> Transform raw, messy data into structured, analysis-ready formats\n",
    "\n",
    "You'll learn to:\n",
    "- Handle missing values strategically\n",
    "- Merge multiple data sources\n",
    "- Fix data types and formats\n",
    "- Address outliers appropriately\n",
    "- Create a clean, production-ready dataset\n",
    "\n",
    "**The Goal**: Turn the \"Not Ready\" mess from Part 1 into a \"Ready\" dataset for AI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from helpers import assess_data_quality, set_plot_style\n",
    "\n",
    "set_plot_style()\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,000,888 sales records\n",
      "Loaded 54 stores\n",
      "Loaded 1,218 oil price records\n",
      "Loaded 350 holiday events\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../data/raw/'\n",
    "\n",
    "train_df = pd.read_csv(DATA_PATH + 'train.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv(DATA_PATH + 'stores.csv')\n",
    "oil_df = pd.read_csv(DATA_PATH + 'oil.csv', parse_dates=['date'])\n",
    "holidays_df = pd.read_csv(DATA_PATH + 'holidays_events.csv', parse_dates=['date'])\n",
    "\n",
    "print(f\"Loaded {train_df.shape[0]:,} sales records\")\n",
    "print(f\"Loaded {stores_df.shape[0]} stores\")\n",
    "print(f\"Loaded {oil_df.shape[0]:,} oil price records\")\n",
    "print(f\"Loaded {holidays_df.shape[0]:,} holiday events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Handle Missing Values\n",
    "\n",
    "### Problem: Oil Prices Missing on Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Missing oil prices: 43\n",
      "\n",
      "After cleaning:\n",
      "Missing oil prices: 0\n",
      "Oil prices cleaned - weekends now use previous trading day price\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning:\")\n",
    "print(f\"Missing oil prices: {oil_df['dcoilwtico'].isnull().sum()}\")\n",
    "\n",
    "# Strategy: Forward fill (use last known price)\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].fillna(method='ffill')\n",
    "\n",
    "# Then backward fill for any remaining nulls at the start\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].fillna(method='bfill')\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(f\"Missing oil prices: {oil_df['dcoilwtico'].isnull().sum()}\")\n",
    "print(f\"Oil prices cleaned - weekends now use previous trading day price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Merge Multiple Data Sources\n",
    "\n",
    "Let's intelligently combine all data sources into one unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting shape: (3000888, 6)\n",
      "After adding stores: (3000888, 10)\n",
      "After adding oil prices: (3000888, 11)\n",
      "Final merged shape: (3000888, 12)\n",
      "All data sources merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# Start with training data\n",
    "df = train_df.copy()\n",
    "\n",
    "print(f\"Starting shape: {df.shape}\")\n",
    "\n",
    "# Add store information\n",
    "df = df.merge(stores_df, on='store_nbr', how='left')\n",
    "print(f\"After adding stores: {df.shape}\")\n",
    "\n",
    "# Add oil prices\n",
    "df = df.merge(oil_df, on='date', how='left')\n",
    "print(f\"After adding oil prices: {df.shape}\")\n",
    "\n",
    "# Process holidays - create binary indicator\n",
    "holidays_national = holidays_df[holidays_df['locale'] == 'National'][['date', 'type']]\n",
    "holidays_national['is_holiday'] = 1\n",
    "holidays_national = holidays_national.drop_duplicates('date')\n",
    "\n",
    "df = df.merge(holidays_national[['date', 'is_holiday']], on='date', how='left')\n",
    "df['is_holiday'] = df['is_holiday'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"Final merged shape: {df.shape}\")\n",
    "print(f\"All data sources merged successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fix Data Types\n",
    "\n",
    "Convert columns to appropriate data types for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types optimized:\n",
      "id                      int64\n",
      "date           datetime64[ns]\n",
      "store_nbr            category\n",
      "family               category\n",
      "sales                 float32\n",
      "onpromotion             int32\n",
      "city                 category\n",
      "state                category\n",
      "type                 category\n",
      "cluster              category\n",
      "dcoilwtico            float64\n",
      "is_holiday              int64\n",
      "dtype: object\n",
      "Memory usage: 131.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical variables\n",
    "categorical_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Ensure numeric types\n",
    "df['sales'] = df['sales'].astype('float32')\n",
    "df['onpromotion'] = df['onpromotion'].astype('int32')\n",
    "\n",
    "print(\"Data types optimized:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Memory savings\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering\n",
    "\n",
    "Create useful features from existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created time-based features:\n",
      "        date  year  month  day  day_of_week  is_weekend  is_holiday\n",
      "0 2013-01-01  2013      1    1            1           0           1\n",
      "1 2013-01-01  2013      1    1            1           0           1\n",
      "2 2013-01-01  2013      1    1            1           0           1\n",
      "3 2013-01-01  2013      1    1            1           0           1\n",
      "4 2013-01-01  2013      1    1            1           0           1\n"
     ]
    }
   ],
   "source": [
    "# Time-based features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_name'] = df['date'].dt.day_name()\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(\"Created time-based features:\")\n",
    "print(df[['date', 'year', 'month', 'day', 'day_of_week', 'is_weekend', 'is_holiday']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Handle Outliers and Zero Sales\n",
    "\n",
    "Let's investigate and address outliers strategically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales statistics:\n",
      "count    3.000888e+06\n",
      "mean     3.577758e+02\n",
      "std      1.101998e+03\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      1.100000e+01\n",
      "75%      1.958473e+02\n",
      "max      1.247170e+05\n",
      "Name: sales, dtype: float64\n",
      "\n",
      "Records with zero sales: 939,130 (31.3%)\n",
      "Zero sales flagged for special handling\n",
      "Days with sales: 2,061,758\n",
      "Days without sales: 939,130\n"
     ]
    }
   ],
   "source": [
    "print(\"Sales statistics:\")\n",
    "print(df['sales'].describe())\n",
    "\n",
    "# Identify zero sales\n",
    "zero_sales = (df['sales'] == 0).sum()\n",
    "print(f\"\\nRecords with zero sales: {zero_sales:,} ({zero_sales/len(df)*100:.1f}%)\")\n",
    "\n",
    "# These zeros are REAL - stores actually had no sales on those days\n",
    "# Keep them, but flag for special handling in modeling\n",
    "df['has_sales'] = (df['sales'] > 0).astype(int)\n",
    "\n",
    "print(f\"Zero sales flagged for special handling\")\n",
    "print(f\"Days with sales: {df['has_sales'].sum():,}\")\n",
    "print(f\"Days without sales: {(1-df['has_sales']).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY ASSESSMENT: CLEANED DATASET\n",
      "============================================================\n",
      "\n",
      "Dataset Shape: 3,000,888 rows × 23 columns\n",
      "\n",
      "Missing Values:\n",
      "            Missing Count  Percentage\n",
      "dcoilwtico         857142   28.562945\n",
      "\n",
      "Data Types:\n",
      "int64             6\n",
      "int32             6\n",
      "datetime64[ns]    1\n",
      "category          1\n",
      "category          1\n",
      "float32           1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "float64           1\n",
      "object            1\n",
      "UInt32            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric Columns: ['id', 'sales', 'onpromotion', 'dcoilwtico', 'is_holiday', 'year', 'month', 'day', 'day_of_week', 'week_of_year', 'quarter', 'is_weekend', 'is_month_start', 'is_month_end', 'has_sales']\n",
      "Categorical Columns: ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'day_name']\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "============================================================\n",
      "\n",
      "Sample of cleaned data:\n",
      "   id       date store_nbr        family  sales  onpromotion   city  \\\n",
      "0   0 2013-01-01         1    AUTOMOTIVE    0.0            0  Quito   \n",
      "1   1 2013-01-01         1     BABY CARE    0.0            0  Quito   \n",
      "2   2 2013-01-01         1        BEAUTY    0.0            0  Quito   \n",
      "3   3 2013-01-01         1     BEVERAGES    0.0            0  Quito   \n",
      "4   4 2013-01-01         1         BOOKS    0.0            0  Quito   \n",
      "5   5 2013-01-01         1  BREAD/BAKERY    0.0            0  Quito   \n",
      "6   6 2013-01-01         1   CELEBRATION    0.0            0  Quito   \n",
      "7   7 2013-01-01         1      CLEANING    0.0            0  Quito   \n",
      "8   8 2013-01-01         1         DAIRY    0.0            0  Quito   \n",
      "9   9 2013-01-01         1          DELI    0.0            0  Quito   \n",
      "\n",
      "       state type cluster  ...  month  day  day_of_week  day_name  \\\n",
      "0  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "1  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "2  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "3  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "4  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "5  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "6  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "7  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "8  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "9  Pichincha    D      13  ...      1    1            1   Tuesday   \n",
      "\n",
      "   week_of_year  quarter is_weekend  is_month_start  is_month_end  has_sales  \n",
      "0             1        1          0               1             0          0  \n",
      "1             1        1          0               1             0          0  \n",
      "2             1        1          0               1             0          0  \n",
      "3             1        1          0               1             0          0  \n",
      "4             1        1          0               1             0          0  \n",
      "5             1        1          0               1             0          0  \n",
      "6             1        1          0               1             0          0  \n",
      "7             1        1          0               1             0          0  \n",
      "8             1        1          0               1             0          0  \n",
      "9             1        1          0               1             0          0  \n",
      "\n",
      "[10 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "assess_data_quality(df, \"CLEANED DATASET\")\n",
    "\n",
    "print(f\"Sample of cleaned data:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned dataset: ../data/processed/sales_cleaned.csv\n",
      "Saved sample dataset: ../data/processed/sales_cleaned_sample.csv\n",
      "Sample size: 42,100 rows (from 3,000,888)\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '../data/processed/'\n",
    "import os\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Save full dataset\n",
    "df.to_csv(OUTPUT_PATH + 'sales_cleaned.csv', index=False)\n",
    "print(f\"Saved cleaned dataset: {OUTPUT_PATH}sales_cleaned.csv\")\n",
    "\n",
    "# Save a focused subset for faster experimentation\n",
    "# Top 5 stores and top 5 product families\n",
    "top_stores = df.groupby('store_nbr')['sales'].sum().nlargest(5).index\n",
    "top_families = df.groupby('family')['sales'].sum().nlargest(5).index\n",
    "\n",
    "df_sample = df[(df['store_nbr'].isin(top_stores)) & (df['family'].isin(top_families))].copy()\n",
    "df_sample.to_csv(OUTPUT_PATH + 'sales_cleaned_sample.csv', index=False)\n",
    "\n",
    "print(f\"Saved sample dataset: {OUTPUT_PATH}sales_cleaned_sample.csv\")\n",
    "print(f\"Sample size: {df_sample.shape[0]:,} rows (from {df.shape[0]:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before vs After Comparison\n",
    "\n",
    "### \"Not Ready\" (Part 1) → \"Ready\" (Now)\n",
    "\n",
    "| Issue | Before | After | Solution |\n",
    "|-------|--------|-------|----------|\n",
    "| **Missing Values** | 43 missing oil prices | 0 missing | Forward fill |\n",
    "| **Scattered Data** | 5 separate files | 1 unified dataset | Strategic merging |\n",
    "| **Data Types** | Inefficient objects | Optimized types | Category conversion |\n",
    "| **Features** | Only raw columns | 15+ engineered features | Time decomposition |\n",
    "| **Zero Sales** | Unknown meaning | Flagged & understood | has_sales indicator |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **Handled Missing Data**: Used domain knowledge to fill gaps appropriately\n",
    "2. **Merged Data Sources**: Combined 5 files into 1 comprehensive dataset\n",
    "3. **Optimized Data Types**: Reduced memory usage and improved efficiency\n",
    "4. **Created Features**: Extracted time-based patterns from dates\n",
    "5. **Validated Quality**: Zero missing values, consistent formats\n",
    "\n",
    "### Skills Demonstrated:\n",
    "- **Pandas** for data manipulation\n",
    "- **NumPy** for numerical operations\n",
    "- Strategic decision-making for missing data\n",
    "- Feature engineering from domain knowledge\n",
    "- Data quality assessment and validation\n",
    "\n",
    "---\n",
    "\n",
    "## Next Step\n",
    "\n",
    "Now that we have clean, structured data, we're ready for **Part 3**:\n",
    "- Visualize sales patterns and trends\n",
    "- Conduct statistical analysis\n",
    "- Identify key drivers of sales\n",
    "- Generate actionable insights\n",
    "\n",
    "**This is where data transforms into decisions.**\n",
    "\n",
    "**Continue to Part 3: Data Exploration & Statistics →**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
